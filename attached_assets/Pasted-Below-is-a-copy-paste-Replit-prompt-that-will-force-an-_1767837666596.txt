Below is a copy/paste Replit prompt that will force an actually-useful Pinecone + namespaces + n8n ingestion + “audio-first then video” conversational runtime.

Also: you’re right about cross-contamination. It’s almost always one of these: missing namespace filter, wrong metadata filter, reused IDs overwriting, or putting summaries from one file into another file’s chunks.

⸻

Replit build prompt (copy/paste)

Build a production-ready multi-avatar RAG system for ELXR using Pinecone namespaces, n8n ingestion, Mem0 long-term memory, and an audio-first conversational runtime (ElevenLabs first, then HeyGen video).

Goals
	1.	Zero cross-contamination across avatars/subjects using Pinecone namespaces + strict allowlists.
	2.	Conversational responses like ChatGPT voice mode: short, spoken, warm, direct, not essay-like.
	3.	Low latency: start speaking via ElevenLabs quickly, then switch to HeyGen video once the final response is ready.
	4.	Ingestion through n8n for thousands of podcasts/courses: chunking + metadata + QA-style “conversation cards” to improve retrieval relevance.
	5.	Mem0 long-term memory works: persists per user and per avatar; restored on login.
	6.	Mandatory disclaimer gate: checkbox acceptance before chat access; store acceptance timestamp per user.

Tech stack
	•	Backend: Node.js + TypeScript + Express
	•	Frontend: React + Vite + TypeScript
	•	Pinecone: single index with multiple namespaces
	•	Mem0: long-term memory
	•	ElevenLabs: TTS streaming
	•	HeyGen: video avatar response (can be stubbed with mock API calls)
	•	n8n: ingestion orchestrator (webhook → backend ingestion endpoints)

⸻

1) Pinecone data model & namespace strategy

Implement one Pinecone index. Use namespaces like:
	•	general_wellness (shared)
	•	sexuality
	•	psychedelics
	•	grief
	•	menopause
	•	addiction
	•	plus optional per-mentor overlays: mentor_thad, mentor_ann (only if needed)

Each avatar has an allowlist of namespaces it is allowed to query. Enforce in code (not prompt).

Every vector record must include metadata:
	•	chunk_id (UUID)
	•	source_id (stable UUID for the original file/episode)
	•	source_title
	•	source_type (podcast|course|book|article|transcript)
	•	speaker (optional)
	•	topics (string[])
	•	audience (string[])
	•	safety_tags (string[])
	•	created_at (ISO)
	•	hash (for dedupe)
	•	chunk_kind = verbatim | conversation_card

IMPORTANT: use unique IDs per namespace so upserts don’t overwrite across namespaces.

⸻

2) Ingestion: “Conversation Cards” + verbatim chunks

For each input file/transcript, create two chunk streams:
A) verbatim chunks (200–450 tokens, ~10–15% overlap)
B) conversation_card chunks generated by LLM from the verbatim chunk:
	•	say_it_like_a_human (2–5 short sentences)
	•	key_points (3–6 bullets)
	•	common_questions (3–8 likely user questions)
	•	watchouts (contraindications / safety notes)

Store both in Pinecone. Retrieve mostly from conversation_card, and use verbatim only when needed for accuracy.

Add a “relevance guard” at ingestion: if the chunk doesn’t match the namespace topic, do not insert it (LLM classification + simple keyword sanity check).

⸻

3) n8n automation

Provide a documented n8n workflow design (JSON export is a bonus) that:
	•	Watches a folder / RSS / uploads
	•	Transcribes audio if needed (stub ok)
	•	Calls backend POST /ingest with { source_id, title, text, source_type, suggested_namespaces }
	•	Backend does: clean → segment → classify namespace → generate conversation cards → embed → upsert to Pinecone
	•	Logs ingestion results and rejects low-confidence chunks

⸻

4) Retrieval & anti-contamination

Implement retrieve(avatarId, userMessage) that:
	•	Uses avatar namespace allowlist
	•	Queries Pinecone per allowed namespace (or multi-namespace loop)
	•	Applies metadata filters: source_type optional, safety_tags optional
	•	Reranks results using a lightweight reranker module (stub ok)
	•	Returns top 3–6 “answer-ready” cards + any required watchouts

Add a debug mode endpoint GET /debug/retrieval?avatarId=...&q=... returning:
	•	namespaces searched
	•	top matches with scores, titles, chunk_kind, topics
so we can catch irrelevant junk fast.

⸻

5) Conversational response engine (like voice mode)

Implement a 2-pass generation:
	1.	Draft: content-correct answer grounded in retrieved cards
	2.	Speech rewrite: rewrite into spoken, conversational tone (short sentences, contractions, light natural phrasing).

Add a “don’t be pedantic” policy: never correct minor naming/typos; only correct if safety-critical.

Topic openness: avatars can discuss any adult topic professionally, but must refuse step-by-step wrongdoing and explicitly note illegality when relevant.

Output format:
	•	1 human opening line
	•	2–4 short paragraphs or bullets
	•	1 gentle next-step question (max 1)
	•	optional safety note only if needed

⸻

6) Low latency: ElevenLabs first, HeyGen second

Implement an “audio-first” pipeline:
	•	Start generating immediately with a fast opener (no retrieval): 1–2 sentences acknowledging the question.
	•	In parallel: retrieval + full answer generation.
	•	Stream TTS to ElevenLabs as tokens arrive.
	•	Once final answer is ready, trigger HeyGen video creation with the final text/audio.

Provide an interface respond(mode) where:
	•	mode="audio" returns streaming audio quickly
	•	mode="video" returns audio immediately + later returns a video URL when ready

(Mocks allowed for ElevenLabs/HeyGen, but code must be structured for real APIs.)

⸻

7) Mem0 long-term memory (must work)

Store memory per {user_id, avatar_id}:
	•	only stable user facts / preferences (max 10 bullets)
	•	plus “open loops” (what the user is working on)
Retrieve memory on login and inject into the prompt as “Known context.”
Do NOT store entire chat logs in Pinecone.

⸻

8) Disclaimer gate (mandatory)

Frontend must show a modal gate on first visit (and optionally each visit):
	•	checkbox: “I understand this is not medical or professional advice…”
	•	button disabled until checked
	•	store acceptance timestamp in backend POST /disclaimer/accept
	•	block access to chat endpoints unless accepted

Provide copy for the disclaimer and store versioning (v1, v2…).

⸻

Deliverables
	•	Full Replit-ready repo
	•	/engine folder with: router.ts, retrieval.ts, prompt.ts, speechRewrite.ts, memory.ts, latency.ts
	•	Example avatar configs in /avatars/*.json including namespace allowlists and personality settings
	•	Example n8n payloads
	•	Debug endpoints + logs
	•	Unit tests for namespace enforcement and cross-contamination prevention

Focus on correctness, modularity, and conversational UX.

⸻

Why your avatars still sound “non-conversational”

You’re doing RAG + “answer writing” but missing a speech rewrite layer.

Do this every time:
	1.	generate a correct answer (grounded)
	2.	rewrite it for speaking (short, human, fewer bullets, fewer formal words)

That second pass is how ChatGPT voice mode feels like a person instead of a brochure.

⸻

Why you’re getting irrelevant knowledge (cross-contamination)

Most common causes (in order):
	•	You’re not filtering by namespace at query time every single request.
	•	Your ingestion uses non-unique IDs so upserts overwrite chunks across topics.
	•	You’re stuffing whole documents or giant chunks that match everything vaguely.
	•	Your n8n flow is sending the wrong namespace or leaving it blank (default namespace = chaos).

The /debug/retrieval endpoint in the prompt above will expose this in 30 seconds.

⸻

Namespace organization that works in practice

Keep it boring and strict:
	•	Topic namespaces (sexuality, psychedelics, grief, etc.)
	•	Optional mentor overlay namespaces only when you truly need “this mentor’s style/content”
	•	A single general_wellness namespace shared by all

Then enforce: avatar → allowlist → retrieval loop. No exceptions.

⸻

Disclaimer copy (you can use this verbatim)

Disclaimer (Required):
ELXR provides educational and informational content and may include personal opinions. It is not medical, legal, or professional advice and is not a substitute for a qualified professional. If you have concerns about your health, safety, or wellbeing, contact a licensed professional or emergency services.

Checkbox: “I understand and agree.”

⸻

If you want, paste:
	•	one example of a “bad” irrelevant retrieval result (question + what it answered + what chunk it pulled), and
	•	your current Pinecone upsert ID strategy,
and I’ll tell you exactly which of the contamination causes you’ve got.